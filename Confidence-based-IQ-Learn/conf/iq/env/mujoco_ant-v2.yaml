# @package _global_


#------- 环境 Env 参数 -------#
env:
  name: Ant-v2
  demo: Ant-v2_25.pkl

  replay_mem: 1e6
  initial_mem: 1280
  horizon: 1000                         # eps_steps/horizon 即智能体与环境交互的最长步骤
  # eps_window: 10
  obs_dim: null

#------- 演示数据参数 -------#
expert:
  demos: 1
  subsample_freq: 1


#------- 训练参数 -------#
train:
  batch: 256

  learn_steps: 1e6
  initial_step: 2e4                     # SAIL 中更新置信度的步数
  eval_interval: 5e3
  roullout_length : 1                   # 每一次更新需要与环境交互采集的transition的数目
  train_steps: 1                        # 每一次更新循环中，价值与策略网络的数目


#------- 评估参数 -------#
eval:
  policy:                               # 测试训练好的策略的文件所在位置 
  threshold: 4500
  horizon: 1000

#------- 算法参数 -------#
agent:
  name: sac
  init_temp: 0.001                      # use a low temp for IL, 初始化自动调节正则化参数alpha

  actor_lr: 3e-05
  actor_update_frequency: 1             # 策略网络更新频率
  num_actor_updates: 1

  critic_lr: 3e-04
  critic_target_update_frequency: 1     # 目标价值网络更新频率

  learn_temp: False                     # learn temperature coefficient 探索系数


#------- 日志参数 -------#
log_interval: 500                       # Log every this many steps



q_net:
  _target_: agent.sac_models.SingleQCritic